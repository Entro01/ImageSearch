{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imagehash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b549b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from io import BytesIO\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "table = dynamodb.Table('LuluHashStore')\n",
    "\n",
    "df = pd.read_csv('image_data.csv')\n",
    "df = df.dropna(subset=['small_image'])\n",
    "\n",
    "async def fetch_image(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        if response.status == 200:\n",
    "            img_data = await response.read()\n",
    "            return Image.open(BytesIO(img_data)).convert('RGB')\n",
    "        else:\n",
    "            raise IOError(f\"Failed to fetch image from {url}, status code: {response.status}\")\n",
    "            \n",
    "async def calculate_phash(session, image_url):\n",
    "    try:\n",
    "        img = await fetch_image(session, image_url)\n",
    "        \n",
    "        long_side = max(img.size)\n",
    "        ratio = 512 / long_side\n",
    "        new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))\n",
    "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        new_img = Image.new('RGB', (512, 512), (255, 255, 255))\n",
    "        paste_pos = ((512 - new_size[0]) // 2, (512 - new_size[1]) // 2)\n",
    "        new_img.paste(img, paste_pos)\n",
    "        \n",
    "        # calculating phash\n",
    "        hash_value = imagehash.phash(new_img)\n",
    "        return str(hash_value)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image at {image_url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "base_url = \"https://d1it09c4puycyh.cloudfront.net\"\n",
    "dimensions = \"355x503\" # most used dimensions (higher the dimensions, the better)\n",
    "\n",
    "async def process_batch(batch_df, batch_number):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        failed_rows = []\n",
    "\n",
    "        for index, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Processing Batch {batch_number}\"):\n",
    "            image_url = f\"{base_url}/{dimensions}/catalog/product{row['small_image'].strip()}\"\n",
    "            phash = await calculate_phash(session, image_url)\n",
    "            if phash is not None:\n",
    "                item = {\n",
    "                    'entity_id': row['entity_id'],\n",
    "                    'sku': row['sku'],\n",
    "                    'small_image': row['small_image'],\n",
    "                    'phash': phash\n",
    "                }\n",
    "                try:\n",
    "                    table.put_item(Item=item)\n",
    "                except ClientError as e:\n",
    "                    print(f\"Failed to insert item into DynamoDB: {e}\")\n",
    "                    failed_rows.append({'entity_id': row['entity_id']})\n",
    "\n",
    "        print(f\"Batch {batch_number} processing complete.\")\n",
    "        if failed_rows:\n",
    "            print(f\"Failed to process {len(failed_rows)} items.\")\n",
    "\n",
    "        gc.collect()  # freeing up memory\n",
    "\n",
    "async def process_all_batches(start_batch=1, end_batch=None):\n",
    "    batch_size = 100\n",
    "    num_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)\n",
    "    \n",
    "    if end_batch is None:\n",
    "        end_batch = num_batches\n",
    "\n",
    "    for i in range(start_batch - 1, end_batch):\n",
    "        batch_df = df.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "        await process_batch(batch_df, i + 1)\n",
    "        \n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_all_batches(start_batch=1, end_batch=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
