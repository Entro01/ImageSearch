import pandas as pd
import imagehash
from PIL import Image
import aiohttp
import asyncio
from io import BytesIO
from tqdm.notebook import tqdm
import boto3
import gc
import json
from botocore.exceptions import ClientError

# Initialize DynamoDB Client
dynamodb = boto3.resource('dynamodb', region_name='your-region')
table = dynamodb.Table('your-dynamodb-table')

# Read CSV
df = pd.read_csv('image_data.csv')
df = df.dropna(subset=['small_image'])

base_url = "https://d1it09c4puycyh.cloudfront.net"
dimensions = "707x1000"

# Image fetching
async def fetch_image(session, url):
    async with session.get(url) as response:
        if response.status == 200:
            img_data = await response.read()
            return Image.open(BytesIO(img_data)).convert('RGB')
        else:
            raise IOError(f"Failed to fetch image from {url}, status code: {response.status}")

# Perceptual hash generation for 707x1000 resolution
def calculate_phash(img):
    target_size = (707, 1000)
    img = img.resize(target_size, Image.Resampling.LANCZOS)
    phash = imagehash.phash(img)
    return str(phash)

# Batch processing function
async def process_batch(batch_df, batch_number):
    async with aiohttp.ClientSession() as session:
        failed_rows = []
        for index, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f"Processing Batch {batch_number}"):
            image_url = f"{base_url}/{dimensions}/catalog/product{row['small_image'].strip()}"
            try:
                img = await fetch_image(session, image_url)
                phash = calculate_phash(img)
                
                item = {
                    'entity_id': row['entity_id'],
                    'sku': row['sku'],
                    'small_image': row['small_image'],
                    'phash': phash
                }
                try:
                    table.put_item(Item=item)  # Insert into DynamoDB
                except ClientError as e:
                    print(f"Failed to insert item into DynamoDB: {e}")
                    failed_rows.append({'entity_id': row['entity_id']})

            except Exception as e:
                print(f"Failed to process image for SKU {row['sku']}: {e}")
                failed_rows.append({'entity_id': row['entity_id']})

        print(f"Batch {batch_number} processing complete.")
        if failed_rows:
            print(f"Failed to process {len(failed_rows)} items.")
        
        gc.collect()  # Freeing up memory

# Main function to process and control batch flow
async def process_all_batches(start_batch=1, end_batch=None):
    batch_size = 100
    num_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)
    
    if end_batch is None:
        end_batch = num_batches

    for i in range(start_batch - 1, end_batch):
        batch_df = df.iloc[i * batch_size:(i + 1) * batch_size]
        await process_batch(batch_df, i + 1)
        gc.collect()

# Function to get the last processed entry from DynamoDB
def get_last_processed_index():
    try:
        # Fetch the last entry in DynamoDB based on the primary key
        response = table.scan(Limit=1, ScanIndexForward=False)
        if response['Items']:
            last_entry = response['Items'][0]
            last_index = df[df['entity_id'] == last_entry['entity_id']].index[0]
            return last_index + 1  # Return the next index to start from
        else:
            return 0  # Start from the beginning if table is empty
    except ClientError as e:
        print(f"Failed to fetch last processed item from DynamoDB: {e}")
        return 0

# Start processing from the last known point
last_processed_index = get_last_processed_index()
start_batch = last_processed_index // 100 + 1  # Determine the batch number to start from

# Process batches from the last known batch onwards
await process_all_batches(start_batch=start_batch)
