{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee34087",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29985041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from io import BytesIO\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# initializing dynamodb connection\n",
    "dynamodb = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "table = dynamodb.Table('LuluFeatureStore')\n",
    "\n",
    "# reading the csv containing the image data\n",
    "df = pd.read_csv('image_data.csv')\n",
    "df = df.dropna(subset=['small_image'])\n",
    "\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # removing final (classification) layer\n",
    "model.eval()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "async def fetch_image(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        if response.status == 200:\n",
    "            img_data = await response.read()\n",
    "            return Image.open(BytesIO(img_data)).convert('RGB')\n",
    "        else:\n",
    "            raise IOError(f\"Failed to fetch image from {url}, status code: {response.status}\")\n",
    "\n",
    "async def extract_features(session, image_url):\n",
    "    try:\n",
    "        img = await fetch_image(session, image_url)\n",
    "        img = preprocess(img).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            features = model(img).flatten().numpy()\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image at {image_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "base_url = \"https://d1it09c4puycyh.cloudfront.net\"\n",
    "dimensions = \"224x224\" # dimensions accepted by resnet50\n",
    "\n",
    "async def process_batch(batch_df, batch_number):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        failed_rows = []\n",
    "\n",
    "        for index, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Processing Batch {batch_number}\"):\n",
    "            image_url = f\"{base_url}/{dimensions}/catalog/product{row['small_image'].strip()}\"\n",
    "            features = await extract_features(session, image_url)\n",
    "            if features is not None:\n",
    "                item = {\n",
    "                    'entity_id': row['entity_id'],\n",
    "                    'sku': row['sku'],\n",
    "                    'small_image': row['small_image'],\n",
    "                    'features': json.dumps(features.tolist())  # serializing features as json string\n",
    "                }\n",
    "                try:\n",
    "                    table.put_item(Item=item) # making a new entry in the table\n",
    "                except ClientError as e:\n",
    "                    print(f\"Failed to insert item into DynamoDB: {e}\")\n",
    "                    failed_rows.append({'entity_id': row['entity_id']})\n",
    "\n",
    "        print(f\"Batch {batch_number} processing complete.\")\n",
    "        if failed_rows:\n",
    "            print(f\"Failed to process {len(failed_rows)} items.\")\n",
    "\n",
    "        gc.collect()  # freeing up memory\n",
    "\n",
    "async def process_all_batches(start_batch=1, end_batch=None):\n",
    "    batch_size = 100\n",
    "    num_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)\n",
    "    \n",
    "    if end_batch is None:\n",
    "        end_batch = num_batches\n",
    "\n",
    "    for i in range(start_batch - 1, end_batch):\n",
    "        batch_df = df.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "        await process_batch(batch_df, i + 1)\n",
    "        \n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce119366",
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_all_batches(start_batch=101, end_batch=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
